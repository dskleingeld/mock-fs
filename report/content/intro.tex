In the last decade we have seen the rise of big data and the shift of everyday data to the cloud. With it rose the need for a highly availible, distributed and scaling file system. The key problem of distributed file systems is how to spread the files. Currently there are two approaches, have a dedicated node to control where each file is placed\footnote{Inspired by GoogleFs\cite{gfs} used by Hadoop filesystem\cite{hdfs} (HDFS) and MooseFs\cite{moosefs}} or have a distribution function decide where a file should be located\footnote{Pioneerd by Ceph\cite{ceph} and also used by GlusterFs\cite{glusterfs}}. The distribution function does not need access to any shared state.

A key problem with the first approaches is the dedicated node, the meta data node, as single point of failure. Implementations such as HDFS have been expanded adding standby nodes that can quickly replace the meta data node. However this requires either access to shared storage\cite{hdfs_ha_nfs}, only moving the point of failure, or a seperate cluster of journalnodes\cite{hdfs_ha_q}.

Here I made a prototype implementing a subset of GoogleFs using the raft\cite{raft} concensus algorithm to turn the one meta node into a failure proof cluster. The cluster has one node responsible for meta-data modifications, the other cluster members activly host consistant read only meta-data. 

In the \cref{sec:impl} I will detail my implementation, then in \cref{sec:res} we test the system. Finally I conclude how well the approache works in \cref{sec:conc}. Instructions on how to deploy the system for testing are in \cref{sec:deploy}.
