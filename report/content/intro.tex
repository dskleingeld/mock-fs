In the last decade we have seen the rise of big data and the shift of everyday data to the cloud. With it rose the need for a highly available, distributed and scaling file system. The key problem of distributed file systems is how to spread the files. Generally there are two approaches, have a dedicated node to control where each file is placed\footnote{Inspired by GoogleFs\cite{gfs} used by Hadoop file-system\cite{hdfs} (HDFS) and MooseFs\cite{moosefs}} or have a distribution function decide where a file should be located\footnote{Pioneered by Ceph\cite{ceph} and also used by GlusterFs\cite{glusterfs}}. The distribution function does not need access to any shared state.

A key problem with the first approaches is the dedicated metadata node, as single point of failure. Implementations such as HDFS have expanded, adding standby nodes that can replace the metadata node. This requires either access to shared storage\cite{hdfs_ha_nfs}, moving the point of failure, or a separate cluster of journal-nodes\cite{hdfs_ha_q}.
-
Here I made a prototype implementing a subset of GoogleFs using the raft\cite{raft} consensus algorithm to turn the one meta node into a failure proof cluster. The cluster has one node responsible for meta-data modifications, the other cluster members actively host consistent read-only meta-data. 

In \cref{sec:impl} I will detail my implementation, then in \cref{sec:res} we test the system. Finally, I discuss how well the approach works in \cref{sec:conc}. Instructions on how to deploy the system for testing are in \cref{sec:deploy}.
