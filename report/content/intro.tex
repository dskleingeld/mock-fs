In the last decade we have seen the rise of big data and the shift of everyday work to the cloud. With it rose the need for a highly available and scaling file system. The problem solved by distributed file systems is how to span, spread and keep consistent files across multiple machines. Generally there are two approaches, a dedicated node to control where each file is placed\footnote{Inspired by GoogleFs\cite{gfs} used by Hadoop file-system\cite{hdfs} (HDFS) and MooseFs\cite{moosefs}} or have a distribution function decide where a file should be located\footnote{Pioneered by Ceph\cite{ceph} and also used by GlusterFs\cite{glusterfs}}. The distribution function does not need access to any shared state, aiding scaling.

A key problem with the first approaches is the dedicated metadata node, as single point of failure. Implementations such as HDFS have expanded, adding standby nodes that can replace the metadata node. This requires either access to shared storage\cite{hdfs_ha_nfs}, moving the point of failure, or a separate cluster of journal-nodes\cite{hdfs_ha_q}.
-
Here I made a prototype implementing a subset of GoogleFs using the raft\cite{raft} consensus algorithm to replace the one meta-data node with a failure proof cluster. The cluster has one node responsible for meta-data modifications, the other cluster members actively host consistent read-only meta-data. 

In \cref{sec:impl} I will detail my implementation, then in \cref{sec:res} we test the system. Finally, I discuss how well this approach works in \cref{sec:conc}. Instructions on how to deploy the system for testing are in \cref{sec:deploy}.
