The file system is consistent and stable under the lightest write and high read loads. Since I did not test other file systems under the same workload I can not conclude wheather the read performance is good. 

In \cref{fig:hbt} we see a make directory load throws off the heartbeat timing of the writeserver resulting in readservers starting elections. The writeserver does not convert to a readserver\footnote{follower in \textit{Raft} terms} which is an implementation error. The readservers, having elected a new leader, no longer acknowledge the writeservers changes which it handles by crashing. This brings the cluster to a consistent state but errors the client connection which stopped the test. 
There are two underlying problems here:

\begin{enumerate}
	\item A load of $5$ requests per second should not throw off heartbeat timing. An increase in heartbeat timeout did not solve the problem pointing to a problem in my consistency model. It might have to do with the way heartbeat messages are canceled and replaced by publish change messages (see \cref{fig:mkdir} \textit{'cancel next heartbeat'}).
	\item The writeserver should put up some \textit{back-pressure} when the load becomes to high. It should stop handling new client requests instead of missing heartbeats.
\end{enumerate}

I also ran into a fundamental issue with the FS. Only a single change can be published to the readservers at the time. If multiple changes are sent concurrently the readserver will eventually receive some changes out of order and conclude it is outdated and go down. 

I can conclude the system, though consistent, is highly unstable during writes. The current implementation is faulty and there is a fundamental problem using raft that limits performance. It seems that read performance scales linearly with cluster size. 

\subsection{Future work}
I think the fundamental issue can be solved with a slight change to \textit{Raft}. In \textit{Raft} a message is rejecting if the previous log index (\textit{prevLogIndex}) of the message is not found in the receivers log. I propose to introduce a pending state in which a message can be partially accepted for some duration. If the previous log index is not found a message will not (yet) be accepted but be put into a pending state until a message with the previous log index is accepted. The message is only acknowledged to the master if one with the missing previous log index is accepted. This should be worked out on paper then implemented.

Further work should add a form of back-pressure and change task scheduling such that heartbeats will be sent on time even under the highest possible load. As we use cooperative multitasking this should be possible.

To quantify performance I need to compare the system to existing solutions such as HDFS. For this a (minimal) data plane needs to be added and a real world workload selected. Then both systems can be benchmarked on the same cluster.

Currently, a readserver that is outdated asks the writeserver for the complete directory. With a large directory and a high amount of changes this can result in an endless loop as the readserver is outdated by the time the writeserver has sent the directory. This can be fixed with the log entry part of \textit{Raft} or another system for incremental updates.

Finally, there are a lot of optimizations possible. For example currently, except for publishing changes, a new connection is started for each request.
